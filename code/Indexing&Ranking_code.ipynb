{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following 2 code blocks are using small file set (100mb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index using mapreduce but not using cluster:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os, re\n",
    "from time import *\n",
    "\n",
    "begin_time = time()    #to get the running time\n",
    "\n",
    "#spark = SparkSession.builder.master('spark://dpw2frankm:7077').appName('SparkInvIndex').getOrCreate()\n",
    "#sc = spark.sparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "\n",
    "inverted_index = sc.wholeTextFiles('hdfs://localhost:9000/user/root/test_small/')\\\n",
    "                    .flatMap(lambda x: [((os.path.basename(x[0]).split(\".\")[0] ,i) ,1) \\\n",
    "                                        for i in re.split('\\\\W', x[1])])\\\n",
    "                    .reduceByKey(lambda a, b: a + b)\\\n",
    "                    .map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "#we skip the collect and print step because it's too slow:\n",
    "output = inverted_index.collect()\n",
    "for i in range(inverted_index.count()): \n",
    "    \n",
    "    print(output[i])\n",
    "    #if i>10:   #limit number to print\n",
    "     #   break\n",
    "\n",
    "end_time = time()    #to get the running time\n",
    "run_time = end_time - begin_time    \n",
    "print('Running time: ', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index using cluster and mapreduce:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os, re\n",
    "from time import *\n",
    "\n",
    "begin_time = time()    #to get the running time\n",
    "\n",
    "spark = SparkSession.builder.master('spark://dpw2frankm:7077').appName('SparkInvIndex').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "#sc = SparkContext.getOrCreate(SparkConf())\n",
    "\n",
    "inverted_index = sc.wholeTextFiles('hdfs://localhost:9000/user/root/test_small/')\\\n",
    "                    .flatMap(lambda x: [((os.path.basename(x[0]).split(\".\")[0] ,i) ,1) \\\n",
    "                                        for i in re.split('\\\\W', x[1])])\\\n",
    "                    .reduceByKey(lambda a, b: a + b)\\\n",
    "                    .map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "#output = inverted_index.collect()\n",
    "#for i in range(inverted_index.count()): \n",
    "    \n",
    " #   print(output[i])\n",
    "  #  if i>10:   #limit number to print\n",
    "   #     break\n",
    "\n",
    "end_time = time()    #to get the running time\n",
    "run_time = end_time - begin_time\n",
    "print('Running time: ', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following 2 code blocks are using large file set (1gb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inverted Index using mapreduce but not using cluster:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os, re\n",
    "from time import *\n",
    "\n",
    "begin_time = time()    #to get the running time\n",
    "\n",
    "#spark = SparkSession.builder.master('spark://dpw2frankm:7077').appName('SparkInvIndex').getOrCreate()\n",
    "#sc = spark.sparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "\n",
    "inverted_index = sc.wholeTextFiles('hdfs://localhost:9000/user/root/ChildrenBooks/')\\\n",
    "                    .flatMap(lambda x: [((os.path.basename(x[0]).split(\".\")[0] ,i) ,1) \\\n",
    "                                        for i in re.split('\\\\W', x[1])])\\\n",
    "                    .reduceByKey(lambda a, b: a + b)\\\n",
    "                    .map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "#we skip the collect and print step because it's too slow:\n",
    "output = inverted_index.collect()\n",
    "for i in range(inverted_index.count()): \n",
    "    \n",
    "    print(output[i])\n",
    "    #if i>100:   #limit number to print\n",
    "     #   break\n",
    "\n",
    "end_time = time()    #to get the running time\n",
    "run_time = end_time - begin_time    \n",
    "print('Running time: ', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running time: ', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index using cluster and mapreduce:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os, re\n",
    "from time import *\n",
    "\n",
    "begin_time = time()    #to get the running time \n",
    "\n",
    "spark = SparkSession.builder.master('spark://dpw2frankm:7077').appName('SparkInvIndex').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "#sc = SparkContext.getOrCreate(SparkConf())\n",
    "\n",
    "inverted_index = sc.wholeTextFiles('hdfs://localhost:9000/user/root/ChildrenBooks/')\\\n",
    "                    .flatMap(lambda x: [((os.path.basename(x[0]).split(\".\")[0] ,i) ,1) \\\n",
    "                                        for i in re.split('\\\\W', x[1])])\\\n",
    "                    .reduceByKey(lambda a, b: a + b)\\\n",
    "                    .map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "#output = inverted_index.collect()\n",
    "#for i in range(inverted_index.count()): \n",
    "    \n",
    " #   print(output[i])\n",
    "  #  if i>100:   #limit number to print\n",
    "   #     break\n",
    "\n",
    "end_time = time()    #to get the running time\n",
    "run_time = end_time - begin_time\n",
    "print('Running time: ', run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking with TF-IDF:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os, re\n",
    "import math\n",
    "\n",
    "#First we compute the TF-IDF of all files in ./iitfidf, save as an RDD\n",
    "#We need SQLContext for processing RDD<->DF\n",
    "#If you want to run the job on Spark cluster, you have to modify the following line, e.g.:\n",
    "spark = SparkSession.builder.master('spark://dpw2tcxu:7077').appName('MyWordCount').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "#sc = SparkContext.getOrCreate(SparkConf())\n",
    "sql = SQLContext(sc)\n",
    "\n",
    "#If you want to run the job with Hadoop HDFS, you have to modify the following line:\n",
    "data = sc.wholeTextFiles('hdfs://localhost:9000/user/root/ChildrenBooks/')\n",
    "\n",
    "numFiles = data.count()  \n",
    "\n",
    "wordcount = data.flatMap(lambda x: [((os.path.basename(x[0]) ,i) ,1) for i in re.split('\\\\W', x[1])])\\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "tf = wordcount.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "idf = wordcount.map(lambda x: (x[0][1], (x[0][0], x[1], 1)))\\\n",
    "        .map(lambda x: (x[0], x[1][2]))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .map(lambda x: (x[0], math.log10(numFiles / x[1])))\n",
    "\n",
    "#Slightly modified map output as (doc, (term, tfidf))\n",
    "tfidf = tf.join(idf)\\\n",
    "            .map(lambda x: (x[1][0][0], (x[0], x[1][0][1] * x[1][1])))\\\n",
    "            .sortByKey()\n",
    "\n",
    "#Then we convert the TF-IDF to an DF, and save to the disk\n",
    "lines = tfidf.map(lambda x: (x[0], x[1][0], x[1][1])).toDF()\n",
    "lines.write.save(\"tfidf-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RDD is mapped as map(lambda x: (x['term'],(x['doc'],x['tfidf']))), for example:\n",
    "#tfidf_RDD = tfidf.map(lambda x: (x[1][0],(x[0],x[1][1])))\n",
    "\n",
    "#Now we load the TF-IDF index from the disk, can convert it to an RDD, and map as x['term'],(x['doc'],x['tfidf'])\n",
    "tfidf_RDD = sql.read.parquet(\"tfidf-index\").rdd.map(lambda x: (x['_2'],(x['_1'],x['_3'])))\n",
    "\n",
    "def tokenize(s):\n",
    "  return re.split(\"\\\\W+\", s.lower())\n",
    "\n",
    "def search(query, topN):\n",
    "  tokens = sc.parallelize(tokenize(query)).map(lambda x: (x, 1) ).collectAsMap()\n",
    "  bcTokens = sc.broadcast(tokens)\n",
    "\n",
    "  #connect to documents with terms in the Query. to Limit the computation space\n",
    "  #so that we don't attempt to compute similarity for docs that have no words in common with our query.  \n",
    "  joined_tfidf = tfidf_RDD.map(lambda x: (x[0], bcTokens.value.get(x[0], '-'), x[1]) ).filter(lambda x: x[1] != '-' )\n",
    "  \n",
    "  #compute the score using aggregateByKey\n",
    "  scount = joined_tfidf.map(lambda a: a[2]).aggregateByKey((0,0),\n",
    "  (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "  (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) )\n",
    "  \n",
    "  scores = scount.map(lambda x: ( x[1][0]*x[1][1]/len(tokens), x[0]) ).top(topN)\n",
    "  \n",
    "  return scores\n",
    "\n",
    "search(\"Little Red Riding-Hood\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
